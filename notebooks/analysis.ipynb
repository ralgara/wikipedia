{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Wikipedia Pageviews Analysis\n",
    "\n",
    "Explore patterns, summary statistics, and trends in Wikipedia pageview data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all JSON files from data/\n",
    "DATA_DIR = Path('../data')\n",
    "\n",
    "def load_pageviews(data_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load all pageview JSON files into a single DataFrame.\"\"\"\n",
    "    records = []\n",
    "    for f in sorted(data_dir.glob('pageviews_*.json')):\n",
    "        with open(f) as fp:\n",
    "            records.extend(json.load(fp))\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    return df\n",
    "\n",
    "df = load_pageviews(DATA_DIR)\n",
    "print(f\"Loaded {len(df):,} records from {df['date'].nunique()} days\")\n",
    "print(f\"Date range: {df['date'].min().date()} to {df['date'].max().date()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filter-header",
   "metadata": {},
   "source": [
    "## Filter Content Pages\n",
    "\n",
    "Exclude special pages (Main_Page, Special:*, User:*, etc.) to focus on actual articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filter-content",
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_CONTENT_PREFIXES = (\n",
    "    'Special:', 'User:', 'Wikipedia:', 'Template:', \n",
    "    'Category:', 'Portal:', 'Draft:', 'Help:', \n",
    "    'Module:', 'MediaWiki:', 'File:', 'TimedText:'\n",
    ")\n",
    "\n",
    "def is_content(article: str) -> bool:\n",
    "    \"\"\"Return True if article is actual content (not a special page).\"\"\"\n",
    "    if article == 'Main_Page':\n",
    "        return False\n",
    "    if article.startswith(NON_CONTENT_PREFIXES):\n",
    "        return False\n",
    "    if '_talk:' in article:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "df['is_content'] = df['article'].apply(is_content)\n",
    "content = df[df['is_content']].copy()\n",
    "\n",
    "print(f\"Content pages: {len(content):,} ({100*len(content)/len(df):.1f}%)\")\n",
    "print(f\"Filtered out: {len(df) - len(content):,} non-content records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily totals\n",
    "daily = content.groupby('date').agg(\n",
    "    total_views=('views', 'sum'),\n",
    "    unique_articles=('article', 'nunique'),\n",
    "    avg_views=('views', 'mean'),\n",
    "    max_views=('views', 'max')\n",
    ").round(0)\n",
    "\n",
    "print(\"Daily Statistics:\")\n",
    "daily.describe().round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "top-articles",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top articles by total views\n",
    "top_overall = content.groupby('article')['views'].sum().sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 20 Articles (by total views across all days):\")\n",
    "top_overall.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trends-header",
   "metadata": {},
   "source": [
    "## Trends Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-views-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total daily views\n",
    "fig, ax = plt.subplots()\n",
    "daily['total_views'].plot(ax=ax, marker='o')\n",
    "ax.set_title('Total Daily Pageviews (Content Pages)')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Views')\n",
    "ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e6:.1f}M'))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "top-articles-over-time",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track top 5 articles over time\n",
    "top5 = top_overall.head(5).index.tolist()\n",
    "top5_daily = content[content['article'].isin(top5)].pivot(\n",
    "    index='date', columns='article', values='views'\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "top5_daily.plot(ax=ax, marker='o')\n",
    "ax.set_title('Top 5 Articles Over Time')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Views')\n",
    "ax.legend(title='Article', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patterns-header",
   "metadata": {},
   "source": [
    "## Interesting Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spikes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find articles with biggest day-over-day spikes\n",
    "# (articles that suddenly became popular)\n",
    "\n",
    "def find_spikes(df, min_views=50000, min_ratio=3.0):\n",
    "    \"\"\"Find articles with sudden popularity spikes.\"\"\"\n",
    "    pivoted = df.pivot(index='date', columns='article', values='views').fillna(0)\n",
    "    \n",
    "    spikes = []\n",
    "    for article in pivoted.columns:\n",
    "        series = pivoted[article]\n",
    "        for i in range(1, len(series)):\n",
    "            prev = series.iloc[i-1]\n",
    "            curr = series.iloc[i]\n",
    "            if prev > 0 and curr >= min_views:\n",
    "                ratio = curr / prev\n",
    "                if ratio >= min_ratio:\n",
    "                    spikes.append({\n",
    "                        'article': article,\n",
    "                        'date': series.index[i],\n",
    "                        'views': int(curr),\n",
    "                        'prev_views': int(prev),\n",
    "                        'spike_ratio': round(ratio, 1)\n",
    "                    })\n",
    "    \n",
    "    return pd.DataFrame(spikes).sort_values('spike_ratio', ascending=False)\n",
    "\n",
    "spikes = find_spikes(content)\n",
    "print(f\"Found {len(spikes)} spike events\")\n",
    "spikes.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rank-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Views vs Rank distribution (log scale)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(content['rank'], content['views'], alpha=0.3, s=5)\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Views vs Rank (Content Pages)')\n",
    "ax.set_xlabel('Rank')\n",
    "ax.set_ylabel('Views (log scale)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "day-of-week",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day of week patterns\n",
    "daily['day_of_week'] = daily.index.day_name()\n",
    "dow_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "daily.groupby('day_of_week')['total_views'].mean().reindex(dow_order).plot(\n",
    "    kind='bar', ax=ax, color='steelblue'\n",
    ")\n",
    "ax.set_title('Average Views by Day of Week')\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('Average Daily Views')\n",
    "ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e6:.1f}M'))\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistency-header",
   "metadata": {},
   "source": [
    "## Consistency Analysis\n",
    "\n",
    "Which articles consistently rank high vs. which are one-hit wonders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Articles that appear in top 100 most consistently\n",
    "n_days = content['date'].nunique()\n",
    "top100_per_day = content[content['rank'] <= 100]\n",
    "\n",
    "consistency = top100_per_day.groupby('article').agg(\n",
    "    days_in_top100=('date', 'nunique'),\n",
    "    avg_rank=('rank', 'mean'),\n",
    "    avg_views=('views', 'mean'),\n",
    "    total_views=('views', 'sum')\n",
    ").sort_values('days_in_top100', ascending=False)\n",
    "\n",
    "consistency['pct_days'] = (100 * consistency['days_in_top100'] / n_days).round(1)\n",
    "\n",
    "print(f\"Most Consistent Top-100 Articles ({n_days} days):\")\n",
    "consistency.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "one-hit-wonders",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hit wonders: high views but only appeared once in top 100\n",
    "one_hit = consistency[\n",
    "    (consistency['days_in_top100'] == 1) & \n",
    "    (consistency['total_views'] > 100000)\n",
    "].sort_values('total_views', ascending=False)\n",
    "\n",
    "print(f\"One-Hit Wonders (top 100 for only 1 day, >100k views):\")\n",
    "one_hit.head(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
